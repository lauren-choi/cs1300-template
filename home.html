<!doctype html>

<html>
  <head>
    <title>CS1300: A/B Testing</title>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,700;1,400;1,500;1,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <meta content="width=device-width, initial-scale=1" name="viewport" />
  </head>

  <body>
    <div class="heading">
      <h1><span class="highlight-1">A/B Testing</span></h1>
      <h3><span class="highlight-15">CSCI 1300 | Fall 2020</span></h3>
      <p>What makes one website better than another? Is it something tangible, like color? Is it the layout? Or is it something more?</p>
      <p>For this project, I designed two versions of a cactus e-commerce website to conduct an A/B test. To measure how differently users responded, I created a Python script to calculate time to completion (t-test) and return rate (chi-squared test).</p>
      <p><a href="https://calm-reef-00329.herokuapp.com" target="_blank">Here's</a> the link to the website (note: versions are randomized on page load).</p>
    </div>

    <div class="section">
      <h2><span class="highlight-2">A versus B</span></h2>
      <p>First, I created two different versions of the same website.</p>

      <div class="versions">
        <div class="version" id="A">
          <h3><span class="highlight-25">Version A</span></h3>
          <img class="website" src="verA.png">
        </div>
  
        <div class="version" id="A">
          <h3><span class="highlight-25">Version B</span></h3>
          <img class="website" src="verB.png">
        </div>
      </div>

      <p>The main differences between the two pages were (1) layout and (2) checkout button location.</p>
      <ul>
        <li><b>Layout:</b> For version A, I laid out the cacti in a vertical scrolling format. In contrast, for version B, I organized the cacti in a grid. This was to see if a spread-out layout versus a grid layout affected time to completion, since the spacing (theoretically) woudl influence how long it took to digest the data.</li>
        <li><b>Checkout Button:</b> For version A, I put the shopping cart at the bottom of the page. For version B, I put it at the top of the page with the heading. This was to see if the location of the shopping cart (which reminds users they have to check out at some point) affected how many items they put in their cart (and thus the return rate if they didn't put enough).</li>
      </ul>
    </div>

    <div class="section">
      <h2><span class="highlight-3">Hypotheses</span></h2>
      <p>Based on the two versions I created, I developed a set of hypotheses for each of the measurements (time to completion and return rate).</p>

      <div class="hypotheses">
        <div class="hypothesis">
          <h3><span class="highlight-35">Time to Completion</span></h2>
          <ul>
            <li><b>Null hypothesis:</b> There will be no difference in the time to completion between Version A and Version B.</li>
            <li><b>Alternative Hypothesis:</b> Version B will have a slightly faster time to completion. This is because Version B's grid layout lets users digest more information at once, rather than having to scroll up and down the page. Additionally, Version B has a more prominent shopping cart location, allowing users to access the checkout page faster.</li>
          </ul>
        </div>
        <div class="hypothesis">
          <h3><span class="highlight-35">Return Rate</span></h2>
          <ul>
            <li><b>Null hypothesis:</b> There will be no difference in the return rate between Version A and Version B.
            </li>
            <li><b>Alternative Hypothesis:</b> Version B will have a slightly higher return rate. This is because Version B's grid layout lets users see all their options at a glance, which increases memorability and learnability. Additionally, Version B has a slightly more compacted view, providing an easier interface for users to add items to their cart without scrolling up and down.
            </li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section">
      <h2><span class="highlight-4">Data Collection Process</span></h2>
      <p>I experienced a slight mishap during data collection -- I forked the wrong template repo, so I didn't have Flask/Heroku embedded in my website during our class's group data collection. As a result, I had trouble fulfilling the 40-user quota; however, I tried to make up for it by sending my website link to my high school friends, my college friends, my family, and posting on Piazza.</p>
    </div>

    <div class="section">
      <h2><span class="highlight-5">Calculations</span></h2>
      <p>For my calculations, I implemented a Python script to calculate the t-score and chi-square value (as well as their p-values) based on a list of data points: one for time to completion, and one for return rate.</p>
      <p>To calculate time to completion, I subtracted the initial page load timestamp from the timestamp of the last page the user visited. I accounted for edge cases like revisiting the same page twice, and I filtered out invalid users.</p>
      <p>To calculate return rate, I counted the number of users who returned and didn't return to the original page after checking out. I also separated them by version A and version B in order to get separate return rates for each version.</p>
    </div>

    <div class="section">
      <h2><span class="highlight-7">Results</span></h2>
      <h3><span class="highlight-75">Results</span></h3>
      <p>The t-test for time to completion produced a p-value of 0.31651, which was greater than my significance level of 0.05. Based on this statistically insignificant information, I failed to reject the null hypothesis; in other words, I accepted that version A and version B have no difference in time to completion.</p>
      <p>The chi-2 test for return rate produced a p-value of 0.03389, which was less than my significance level of 0.05. Based on this statistically significant information, I rejected the null hypothesis; in other words, I accepted that version B has a higher return rate than version A.</p>

      <h3><span class="highlight-75">Infographic</span></h3>
      <p>Using my results, I then designed an infographic to create a visual representation of my A/B testing results.</p>
      <img src="AB-testing-infographic.svg" id="infographic">

      <h3><span class="highlight-75">Limitations and Implications</span></h3>
      <p>I also identified the limitations of my user study and what the results told me about design principles in general.</p>
      <p><b>Limitations</b></p>
      <ul>
        <li><b>Small sample size:</b> the development error prevented me from reaching 40 users, but I would have liked a larger sample size to avoid possibly skewing the data.</li>
      </ul>
      <p><b>Implications</b></p>
      <ul>
        <li><b>Return rate is based on presentation, not content.</b> Version A and version B contained the exact same information. The only difference was in presentation; version B had a slightly compacted grid layout (allowing users to digest information at a glance), while version A leaned more towards a scrolling layout. I believe return rate -- the user's willingness to further explore a page -- depended heavily on memorability, and because version B had a higher return rate, I've learned presentation and first impressions influence users more than I thought.</li>
        <li><b>Navigation has little to no effect on time to completion.</b>  I was surprised that time to completion produced statistically insignificant results -- I thought version B would be a shoo-in for faster time to completion, especially since everything was so compacted. However, my results helped me realize that at the end of the day, ease of navigation can only get you so far. Instead, what impacts time to completion is content: how much information do users have to digest?</li>
      </ul>
    </div>

    <div class="section">
      <h2><span class="highlight-8">Conclusion</span></h2>
      <p>Overall, this project was a great exercise in the quantitative side of user research. Until now, I'd only experienced the qualitiative aspect of UIUX (prototyping, sketching, sending surveys, etc.), so I enjoyed learning how numbers and statistics measure user experience. It's hard to quantify something like "what makes this website better than the other?", and A/B testing definitely removes some of that mystery.</p>
    </div>
  </body>
</html>
